{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 该代码提供方法示例，涉及到数据安全，所以不代表最终的实现。请供参考。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import faiss\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "graph_path = 'your graph.csv'\n",
    "chunks_path = 'your chunks.csv'\n",
    "# 读取 graph.csv 文件\n",
    "graph_df = pd.read_csv(graph_path, delimiter='|', names=['node_1', 'node_2', 'edge', 'chunk_id'])\n",
    "\n",
    "# 读取 chunks.csv 文件\n",
    "chunks_df = pd.read_csv(chunks_path, delimiter='|', names=['text', 'source', 'chunk_id'])\n",
    "\n",
    "graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings as BaseEmbeddings\n",
    "from typing import List\n",
    "\n",
    "class SentenceTransformerEmbeddings(BaseEmbeddings):\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, convert_to_tensor=False).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode([text], convert_to_tensor=False).tolist()[0]\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name='your model name')\n",
    "embedding = embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为每一行生成Embedding并存储到列表\n",
    "embeddings = []\n",
    "triplets = []\n",
    "for index, row in graph_df.iterrows():\n",
    "    triplet_text = f\"({row['node_1']} , {row['edge']} , {row['node_2']})\"\n",
    "    triplet_embedding = embedding_model.embed_query(triplet_text)\n",
    "    embeddings.append((triplet_embedding, row['chunk_id']))\n",
    "    triplets.append(triplet_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将生成的向量存储到FAISS向量数据库\n",
    "dimension = len(embeddings[0][0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# 存储chunk_id的列表\n",
    "chunk_ids = []\n",
    "\n",
    "for embedding, chunk_id in embeddings:\n",
    "    embedding_array = np.array(embedding, dtype=np.float32).reshape(1, -1)  # 确保是float32类型的二维数组\n",
    "    index.add(embedding_array)\n",
    "    chunk_ids.append(chunk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存索引和chunk_id\n",
    "faiss.write_index(index, 'vector_index.faiss')\n",
    "with open('chunk_ids.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([[cid] for cid in chunk_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 检查是否已经存在保存的索引和chunk_id文件\n",
    "index_file = 'vector_index.faiss'\n",
    "chunk_ids_file = 'chunk_ids.csv'\n",
    "\n",
    "def load_or_create_index():\n",
    "    if os.path.exists(index_file) and os.path.exists(chunk_ids_file):\n",
    "        # 如果文件存在，直接读取\n",
    "        index = faiss.read_index(index_file)\n",
    "        with open(chunk_ids_file, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)  # 跳过第一行（列名）\n",
    "            chunk_ids = [row[0] for row in reader]  # 直接读取字符串\n",
    "        triplets = []\n",
    "        for indexs,row in graph_df.iterrows():\n",
    "            triplet_text = f\"({row['node_1']} , {row['edge']} , {row['node_2']})\"\n",
    "            triplets.append(triplet_text)\n",
    "    else:\n",
    "        # 如果文件不存在，生成并保存嵌入向量和chunk_id\n",
    "        embeddings = []\n",
    "        triplets = []\n",
    "        for index, row in graph_df.iterrows():\n",
    "            triplet_text = f\"({row['node_1']} , {row['edge']} , {row['node_2']})\"\n",
    "            triplet_embedding = embedding_model.embed_query(triplet_text)\n",
    "            embeddings.append((triplet_embedding, row['chunk_id']))\n",
    "            triplets.append(triplet_text)\n",
    "\n",
    "        # 将生成的向量存储到FAISS向量数据库\n",
    "        dimension = len(embeddings[0][0])\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "        # 存储chunk_id的列表\n",
    "        chunk_ids = []\n",
    "\n",
    "        for embedding, chunk_id in embeddings:\n",
    "            embedding_array = np.array(embedding, dtype=np.float32).reshape(1, -1)  # 确保是float32类型的二维数组\n",
    "            index.add(embedding_array)\n",
    "            chunk_ids.append(str(chunk_id))  # 确保chunk_id是字符串\n",
    "\n",
    "        # 保存索引和chunk_id\n",
    "        faiss.write_index(index, index_file)\n",
    "        with open(chunk_ids_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['chunk_id'])  # 写入列名\n",
    "            writer.writerows([[cid] for cid in chunk_ids])\n",
    "    \n",
    "    return index, chunk_ids,triplets\n",
    "\n",
    "# 加载或创建索引\n",
    "index, chunk_ids ,triplets= load_or_create_index()\n",
    "\n",
    "# 步骤2: 向量相似度查询\n",
    "def query_database(querys, index=index, chunk_ids=chunk_ids,triplets=triplets):\n",
    "    unique_triplets = set()\n",
    "    unique_chunk_ids = set()\n",
    "    for query in querys:\n",
    "        query_embedding = embedding_model.embed_query(query)\n",
    "        query_embedding_array = np.array(query_embedding, dtype=np.float32).reshape(1, -1)  # 确保是float32类型的二维数组\n",
    "        D, I = index.search(query_embedding_array, k=5)  # 找到最相似的5个向量\n",
    "        similar_chunk_ids = [chunk_ids[i] for i in I[0] if i < len(chunk_ids)]  # 检查索引有效性\n",
    "        unique_chunk_ids.update(similar_chunk_ids)                                       \n",
    "\n",
    "        # 步骤4: 查找对应的三元组并添加到集合\n",
    "        matching_triplets = [triplets[i] for i in I[0] if i < len(triplets)]  # 检查索引有效性\n",
    "        unique_triplets.update(matching_triplets)\n",
    "\n",
    "    matching_chunks = chunks_df[chunks_df['chunk_id'].isin(unique_chunk_ids)]['text']\n",
    "\n",
    "    return unique_triplets, matching_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings,ChatOpenAI\n",
    "#可替换为本地模型\n",
    "os.environ['OPENAI_API_KEY'] = \"your api key\"\n",
    "os.environ['OPENAI_API_BASE'] = \"your api base\"\n",
    "llm = ChatOpenAI(model='local_model or online_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "import re\n",
    "def rewrite_and_split_query(query):\n",
    "    # 使用智能体来重写和拆分查询\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=\"\"\"\n",
    "        你是个智能体助手，我给你给你一个query，你需要把问题拆解为便于用于在知识图谱向量数据库中查询的短语，提升在向量数据库中匹配的准确率。\n",
    "        你只需要回答出拆解后的短语即可，不要添加或者修改，中文回答。\n",
    "        接下来给你一个例子：\n",
    "        例子1:\n",
    "            问题:\n",
    "                在直动式顺序阀是如何通过控制油液压力来实现缸Ⅰ和缸Ⅱ按顺序动作的？\n",
    "            短语: \n",
    "                直通式顺序阀\\n\n",
    "                油液压力\\n\n",
    "                缸Ⅰ和缸Ⅱ按顺序动作\\n\n",
    "        例子2：\n",
    "            问题:\n",
    "                在液压系统中，如何通过故障诊断来确定并解决‘系统压力波动’的问题？\n",
    "            短语：\n",
    "                液压系统\\n\n",
    "                故障诊断\\n\n",
    "                系统压力波动\\n\n",
    "        请把下面的问题拆解为短语，你必须严格按照上述规定的短语格式输出。\n",
    "        问题: {query}\n",
    "        短语:\n",
    "        \"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(query)\n",
    "    sub_queries = result.split('\\n')  # 假设结果是按行分隔的\n",
    "    sub_queries.append(query)  # 最后一行是query本身\n",
    "    return sub_queries\n",
    "\n",
    "# 步骤2: 优化查询结果并回答\n",
    "def optimize_contexts(query,contexts):\n",
    "    # 利用智能体优化三元组和chunks\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query, contexts\"],\n",
    "        template=\"\"\"\n",
    "        你是一个智能体助手，我给你一个query和一些相关的上下文contexts,由于上下文过长，你需要从中提炼出对于解答query相关的，直接的，有用的信息。\n",
    "        query: {query}\n",
    "        contexts: {contexts}\n",
    "        首先你需要对query进行分析，明确query的意图，以及分析出解答query所需要的可能的信息。\n",
    "        然后需要对contexts进行分析，提炼出所有有用的信息，最终输出一个优化的上下文，用来解答query。输出的结果需要包含三元组和context，确保这些三元组和context是与query相关的。\n",
    "        你需要输出三元组及其对三元组的解释，以及context。\n",
    "        你只需要提炼出与query相关的信息！！！尽量保持原文信息一致性，不用描述和概括。\n",
    "        不允许出现英文回答，只能用中文回答。\n",
    "        你的输出必须符合下述格式！不允许输出别的内容。\n",
    "        请一句一句分析，不能跳过任何一句，一步一步思考。\n",
    "        输出格式如下：\n",
    "        Triplets:\n",
    "        (node1,relation,node2)，后面是对形成三元组和解答query有用的信息的解释。\n",
    "        Context: \n",
    "        优化提炼之后的上下文。\n",
    "        请开始提炼:\n",
    "        \"\"\"\n",
    "    )\n",
    "    input = {\n",
    "        \"query\": query,\n",
    "        \"contexts\": contexts\n",
    "    }\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    optimized_contexts = chain.run(input)\n",
    "    \n",
    "    return optimized_contexts\n",
    "\n",
    "# 步骤3: 思考，纠正，再次优化\n",
    "def think_And_reflect(query,contexts,n=3):\n",
    "    # 利用智能体优化三元组和chunks\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query, contexts, n\"],\n",
    "        template=\"\"\"\n",
    "        你是一个世界级的人工智能系统，能够进行复杂且准确的推理和反思，经过{n}次迭代的thinking和reflection，最终得到[answer]。\n",
    "        query: {query}\n",
    "        contexts: {contexts}\n",
    "        首先在[thinking]标签下进行推理，然后在[reflection]标签反思你的回答，分析可能出现的错误和能够改进的地方，并重复这个过程，反复进行{n}轮的[thinking]和[reflection]。最后，你得到了[answer]。\n",
    "        请注意，我提供的contexts里面包含三元组Triplets以及context内容，内容可能和query相关，但不一定是直接的答案。所以你需要一一思考，反思，再次优化，最终得到一个优化的答案。\n",
    "        你的分析过程和最终answer必须来自于contexts，不能捏造和歪曲！\n",
    "        请一步一步认真思考，最后结果用中文输出。\n",
    "        你必须按照以下格式输出！\n",
    "        输出格式如下：\n",
    "        第一轮迭代:\n",
    "        [thinking]\n",
    "        你的推理过程。\n",
    "        [reflection]\n",
    "        你的反思过程。\n",
    "        第二轮迭代:\n",
    "        [thinking]\n",
    "        你的推理过程。\n",
    "        [reflection]\n",
    "        你的反思过程。\n",
    "        ......\n",
    "        [answer]：\n",
    "        你最终的答案，你只需要回答问题的答案即可，不用添加其他描述，例如“根据上下文”、“根据以上分析”等语句。\n",
    "        确保要回答到问题中的每个要点，答案不需要分点回答。要表现的像一个标准答案。\n",
    "        接下来，请一步一步思考，并且必须按照格式输出！回答的最后一定要包含[answer]！\n",
    "        \"\"\"\n",
    "    )\n",
    "    input = {\n",
    "        \"query\": query,\n",
    "        \"contexts\": contexts,\n",
    "        \"n\": n\n",
    "    }\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    output = chain.run(input)\n",
    "    print(output)\n",
    "    # 定义正则表达式模式\n",
    "    pattern = r'\\[answer\\]\\s*(.*?)\\s*(?=\\[|\\Z)'\n",
    "\n",
    "    match = re.search(pattern, output, re.DOTALL)\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(query,context):\n",
    "    prompt = PromptTemplate(\n",
    "    input_variables=[\"query, contexts\"],\n",
    "    template=\"\"\"\n",
    "    你是一个智能体助手，我给你一个query和一些相关的上下文contexts,由于上下文过长，你需要从中提炼出对于解答query相关的，直接的，有用的信息。\n",
    "    query: {query}\n",
    "    contexts: {contexts}\n",
    "    首先你需要对query进行分析，明确query的意图，以及分析出解答query所需要的可能的信息。\n",
    "    然后需要对contexts进行分析，提炼出所有有用的信息，最终输出一个优化的上下文，用来解答query。输出的结果需要包含三元组和context，确保这些三元组和context是与query相关的。\n",
    "    你需要输出三元组及其对三元组的解释，以及context。\n",
    "    你只需要提炼出与query相关的信息！！！尽量保持原文信息一致性，不用描述和概括。\n",
    "    不允许出现英文回答，只能包含英文。\n",
    "    你的输出必须符合下述格式！不允许输出别的内容。\n",
    "    请一句一句分析，不能跳过任何一句，一步一步思考。\n",
    "    输出格式如下：\n",
    "    Triplets:\n",
    "    (node1,relation,node2)，后面是对形成三元组和解答query有用的信息的解释。\n",
    "    Context: \n",
    "    优化提炼之后的上下文。\n",
    "    请开始提炼:\n",
    "    \"\"\"\n",
    "    )\n",
    "    input = {\n",
    "        'query': query,\n",
    "        'contexts': context\n",
    "    }\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    output = chain.run(input)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "query= df['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求rag系统平均时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "# Combine all contexts into a single string\n",
    "#final_context = \"\\n\\n\".join(all_contexts)\n",
    "#print(final_context)\n",
    "\n",
    "# Create prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "根据以下提供的Triplets,context的信息来回答问题：\n",
    "information:{context}\n",
    "根据上述上下文回答问题：\n",
    "query：{question}。\n",
    "首先分析问题，明确解答问题的信息，然后在提供的information中寻找答案，请一步一步思考。\n",
    "答案不要分段和分点！！！\n",
    "必须完整的回答问题，包括query中可能的多个问题！\n",
    "答案必须和query高度相关！确保是能够回答问题的最直接、准确、明了的答案。\n",
    "不要为你的答案提供理由！！！\n",
    "不要提供上下文信息中未提到的信息，不要胡编乱造。\n",
    "请提供与解决问题最直接相关的答案，不需要推理过程！！！\n",
    "不要说“根据上下文”或“在上下文中提到”或类似的话。\n",
    "你只需要生成回答的答案，不能出现别的无关的语句！！！\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "total_time = 0\n",
    "query_count = 0\n",
    "for q in query:\n",
    "    start_time = time.time() \n",
    "    sub_q = rewrite_and_split_query(q)\n",
    "    triplex,chunks = query_database(sub_q)\n",
    "    triples = \"\\n\".join(triplex)\n",
    "    text = ''\n",
    "    for content in chunks:\n",
    "        text += str(content)\n",
    "    combined_context = f\"Triplets:\\n{triplex}\\n\\nContext:\\n{text}\"\n",
    "    final_context = optimize_contexts(q, combined_context)\n",
    "    prompt = prompt_template.format(context=final_context, question=q)\n",
    "    model = llm \n",
    "    response_text = model.predict(prompt)\n",
    "    end_time = time.time()  # 记录结束时间\n",
    "    elapsed_time = end_time - start_time  # 计算单次查询的时间\n",
    "    total_time += elapsed_time\n",
    "    query_count += 1\n",
    "\n",
    "    # print(f\"Query: {q}\")\n",
    "    # print(f\"Response: {response_text}\")\n",
    "    print(f\"Time taken: {elapsed_time:.4f} seconds\\n\")\n",
    "\n",
    "average_time = total_time / query_count  # 计算平均时间\n",
    "print(f\"Average time taken per query: {average_time:.4f} seconds\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "all_contexts = []\n",
    "\n",
    "# Loop through each query, retrieve context, and store it\n",
    "for q in query:\n",
    "    sub_querys =rewrite_and_split_query(q)\n",
    "    tripes, chunks = query_database(sub_querys)\n",
    "    triples = \"\\n\".join(tripes)\n",
    "    text = ''\n",
    "    for content in chunks:\n",
    "        text += str(content)\n",
    "    combined_context = f\"Triplets:\\n{tripes}\\n\\nContext:\\n{text}\"\n",
    "    final_context = optimize_contexts(q, combined_context)\n",
    "    all_contexts.append(final_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for i, q in enumerate(query):\n",
    "    prompt = prompt_template.format(context=all_contexts[i], question=q)\n",
    "    model = llm \n",
    "    response_text = model.predict(prompt)\n",
    "    responses.append(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = responses\n",
    "df[\"retrival_contexts\"] = all_contexts\n",
    "print(\"Existing columns:\", df.columns)\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
