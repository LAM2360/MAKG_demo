{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEMO FOR MAKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import faiss\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "graph_path = 'your graph.csv'\n",
    "chunks_path = 'your chunks.csv'\n",
    "# 读取 graph.csv 文件\n",
    "graph_df = pd.read_csv(graph_path, delimiter='|', names=['node_1', 'node_2', 'edge', 'chunk_id'])\n",
    "\n",
    "# 读取 chunks.csv 文件\n",
    "chunks_df = pd.read_csv(chunks_path, delimiter='|', names=['text', 'source', 'chunk_id'])\n",
    "\n",
    "graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings as BaseEmbeddings\n",
    "from typing import List\n",
    "\n",
    "class SentenceTransformerEmbeddings(BaseEmbeddings):\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, convert_to_tensor=False).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode([text], convert_to_tensor=False).tolist()[0]\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name='your model name')\n",
    "embedding = embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为每一行生成Embedding并存储到列表\n",
    "embeddings = []\n",
    "triplets = []\n",
    "for index, row in graph_df.iterrows():\n",
    "    triplet_text = f\"({row['node_1']} , {row['edge']} , {row['node_2']})\"\n",
    "    triplet_embedding = embedding_model.embed_query(triplet_text)\n",
    "    embeddings.append((triplet_embedding, row['chunk_id']))\n",
    "    triplets.append(triplet_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将生成的向量存储到FAISS向量数据库\n",
    "dimension = len(embeddings[0][0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# 存储chunk_id的列表\n",
    "chunk_ids = []\n",
    "\n",
    "for embedding, chunk_id in embeddings:\n",
    "    embedding_array = np.array(embedding, dtype=np.float32).reshape(1, -1)  # 确保是float32类型的二维数组\n",
    "    index.add(embedding_array)\n",
    "    chunk_ids.append(chunk_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存索引和chunk_id\n",
    "faiss.write_index(index, 'vector_index.faiss')\n",
    "with open('chunk_ids.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([[cid] for cid in chunk_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# 初始化\n",
    "knowledge_graph = nx.DiGraph()\n",
    "# 添加边的方法\n",
    "knowledge_graph.add_edges_from([(\"n1\", \"n2\"), (\"n2\", \"n3\"), (\"n3\", \"n1\")])  \n",
    "\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    计算两个向量的余弦相似度\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    return dot_product / (norm_v1 * norm_v2)\n",
    "\n",
    "# 检索二跳邻居节点\n",
    "def extract_subgraph(key_node):\n",
    "    \"\"\"\n",
    "    根据关键节点提取子图（包含关键节点、一跳节点、二跳节点）的函数\n",
    "    \"\"\"\n",
    "\n",
    "    # 相似度最高的节点\n",
    "    max_similarity_node = key_node\n",
    "\n",
    "    # 提取一跳节点\n",
    "    one_hop_nodes = set()\n",
    "    for edge in knowledge_graph.edges():\n",
    "        if max_similarity_node in edge:\n",
    "            other_node = edge[0] if edge[1] == max_similarity_node else edge[1]\n",
    "            one_hop_nodes.add(other_node)\n",
    "\n",
    "    # 提取二跳节点\n",
    "    two_hop_nodes = set()\n",
    "    for one_hop_node in one_hop_nodes:\n",
    "        adjacent_nodes = set(knowledge_graph.neighbors(one_hop_node))\n",
    "        adjacent_nodes.discard(max_similarity_node)\n",
    "        two_hop_nodes.update(adjacent_nodes)\n",
    "\n",
    "    # 构建子图（这里将关键节点、一跳节点、二跳节点合并作为子图的节点集合）\n",
    "    subgraph_nodes = {max_similarity_node} | one_hop_nodes | two_hop_nodes\n",
    "    subgraph = knowledge_graph.subgraph(subgraph_nodes)\n",
    "    return subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 检查是否已经存在保存的索引和chunk_id文件\n",
    "index_file = 'vector_index.faiss'\n",
    "chunk_ids_file = 'chunk_ids.csv'\n",
    "\n",
    "def load_or_create_index():\n",
    "    if os.path.exists(index_file) and os.path.exists(chunk_ids_file):\n",
    "        # 如果文件存在，直接读取\n",
    "        index = faiss.read_index(index_file)\n",
    "        with open(chunk_ids_file, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)  # 跳过第一行（列名）\n",
    "            chunk_ids = [row[0] for row in reader]  # 直接读取字符串\n",
    "        triplets = []\n",
    "        for indexs,row in graph_df.iterrows():\n",
    "            triplet_text = f\"({row['node_1']} , {row['edge']} , {row['node_2']})\"\n",
    "            triplets.append(triplet_text)\n",
    "    else:\n",
    "        # 如果文件不存在，生成并保存嵌入向量和chunk_id\n",
    "        embeddings = []\n",
    "        triplets = []\n",
    "        for index, row in graph_df.iterrows():\n",
    "            triplet_text = f\"({row['node_1']} , {row['edge']} , {row['node_2']})\"\n",
    "            triplet_embedding = embedding_model.embed_query(triplet_text)\n",
    "            embeddings.append((triplet_embedding, row['chunk_id']))\n",
    "            triplets.append(triplet_text)\n",
    "\n",
    "        # 将生成的向量存储到FAISS向量数据库\n",
    "        dimension = len(embeddings[0][0])\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "        # 存储chunk_id的列表\n",
    "        chunk_ids = []\n",
    "\n",
    "        for embedding, chunk_id in embeddings:\n",
    "            embedding_array = np.array(embedding, dtype=np.float32).reshape(1, -1)  # 确保是float32类型的二维数组\n",
    "            index.add(embedding_array)\n",
    "            chunk_ids.append(str(chunk_id))  # 确保chunk_id是字符串\n",
    "\n",
    "        # 保存索引和chunk_id\n",
    "        faiss.write_index(index, index_file)\n",
    "        with open(chunk_ids_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['chunk_id'])  # 写入列名\n",
    "            writer.writerows([[cid] for cid in chunk_ids])\n",
    "    \n",
    "    return index, chunk_ids,triplets\n",
    "\n",
    "# 加载或创建索引\n",
    "index, chunk_ids ,triplets= load_or_create_index()\n",
    "\n",
    "# 步骤2: 向量相似度查询\n",
    "def query_database(querys, index=index, chunk_ids=chunk_ids,triplets=triplets):\n",
    "    unique_triplets = set()\n",
    "    unique_chunk_ids = set()\n",
    "    for query in querys:\n",
    "        query_embedding = embedding_model.embed_query(query)\n",
    "        query_embedding_array = np.array(query_embedding, dtype=np.float32).reshape(1, -1)  # 确保是float32类型的二维数组\n",
    "        D, I = index.search(query_embedding_array, k=5)  # 找到最相似的5个向量\n",
    "        similar_chunk_ids = [chunk_ids[i] for i in I[0] if i < len(chunk_ids)]  # 检查索引有效性\n",
    "        unique_chunk_ids.update(similar_chunk_ids)                                       \n",
    "\n",
    "        # 步骤4: 查找对应的三元组并添加到集合\n",
    "        matching_triplets = [triplets[i] for i in I[0] if i < len(triplets)]  # 检查索引有效性\n",
    "        unique_triplets.update(matching_triplets)\n",
    "\n",
    "    matching_chunks = chunks_df[chunks_df['chunk_id'].isin(unique_chunk_ids)]['text']\n",
    "    sub_graphs = extract_subgraph(unique_triplets)\n",
    "\n",
    "    return sub_graphs, matching_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings,ChatOpenAI\n",
    "#可替换为本地模型\n",
    "os.environ['OPENAI_API_KEY'] = \"your api key\"\n",
    "os.environ['OPENAI_API_BASE'] = \"your api base\"\n",
    "llm = ChatOpenAI(model='local_model or online_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "import re\n",
    "def rewrite_and_split_query(query):\n",
    "    # 使用智能体来重写和拆分查询\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=\"\"\"\n",
    "        你是个智能体助手，我给你给你一个query，你需要把问题拆解为便于用于在知识图谱向量数据库中查询的短语，提升在向量数据库中匹配的准确率。\n",
    "        你只需要回答出拆解后的短语即可，不要添加或者修改，中文回答。\n",
    "        接下来给你一个例子：\n",
    "        例子1:\n",
    "            问题:\n",
    "                在直动式顺序阀是如何通过控制油液压力来实现缸Ⅰ和缸Ⅱ按顺序动作的？\n",
    "            短语: \n",
    "                直通式顺序阀\\n\n",
    "                油液压力\\n\n",
    "                缸Ⅰ和缸Ⅱ按顺序动作\\n\n",
    "        例子2：\n",
    "            问题:\n",
    "                在液压系统中，如何通过故障诊断来确定并解决‘系统压力波动’的问题？\n",
    "            短语：\n",
    "                液压系统\\n\n",
    "                故障诊断\\n\n",
    "                系统压力波动\\n\n",
    "        请把下面的问题拆解为短语，你必须严格按照上述规定的短语格式输出。\n",
    "        问题: {query}\n",
    "        短语:\n",
    "        \"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(query)\n",
    "    sub_queries = result.split('\\n')  # 假设结果是按行分隔的\n",
    "    sub_queries.append(query)  # 最后一行是query本身\n",
    "    return sub_queries\n",
    "\n",
    "# 步骤2: 优化查询结果并回答\n",
    "def optimize_contexts(query,contexts):\n",
    "    # 利用智能体优化三元组和chunks\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query, contexts\"],\n",
    "        template=\"\"\"\n",
    "        你是一个智能体助手，我给你一个query和一些相关的上下文contexts,由于上下文过长，你需要从中提炼出对于解答query相关的，直接的，有用的信息。\n",
    "        query: {query}\n",
    "        contexts: {contexts}\n",
    "        首先你需要对query进行分析，明确query的意图，以及分析出解答query所需要的可能的信息。\n",
    "        然后需要对contexts进行分析，提炼出所有有用的信息，最终输出一个优化的上下文，用来解答query。输出的结果需要包含三元组和context，确保这些三元组和context是与query相关的。\n",
    "        你需要输出三元组及其对三元组的解释，以及context。\n",
    "        你只需要提炼出与query相关的信息！！！尽量保持原文信息一致性，不用描述和概括。\n",
    "        不允许出现英文回答，只能用中文回答。\n",
    "        你的输出必须符合下述格式！不允许输出别的内容。\n",
    "        请一句一句分析，不能跳过任何一句，一步一步思考。\n",
    "        输出格式如下：\n",
    "        Triplets:\n",
    "        (node1,relation,node2)，后面是对形成三元组和解答query有用的信息的解释。\n",
    "        Context: \n",
    "        优化提炼之后的上下文。\n",
    "        请开始提炼:\n",
    "        \"\"\"\n",
    "    )\n",
    "    input = {\n",
    "        \"query\": query,\n",
    "        \"contexts\": contexts\n",
    "    }\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    optimized_contexts = chain.run(input)\n",
    "    \n",
    "    return optimized_contexts\n",
    "\n",
    "# 步骤3: 思考，纠正，再次优化\n",
    "goal = 0.85 #设定一个评分阈值，如果子问题答案评分小于这个阈值，则继续迭代评估\n",
    "def evaluate_answers(sub_questions, total_answer, reflect = \"\") -> str:\n",
    "    \"\"\"\n",
    "    根据子问题和总问题答案评估子问题答案评分，并计算综合得分的函数\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    weights = []\n",
    "    eval = \"\"\n",
    "    for sub_q_info in sub_questions:\n",
    "        sub_question = sub_q_info[\"question\"]\n",
    "        weight = sub_q_info[\"weight\"]\n",
    "        # 构造提示语，让LLM去评估子问题答案与总答案的匹配度等情况给出评分（这里评分范围假设是0到1），实际上可以提供适当的例子和提示，让LLM更好地理解评分的意义\n",
    "        prompt = f\"请根据总问题的答案：{total_answer}，评估子问题 '{sub_question}' 的答案符合程度，并给出一个0到1之间的评分，越符合评分越高。并且请放回你的的评价。\"\n",
    "        chain = LLMChain(llm=llm)\n",
    "        response = chain.run(prompt)\n",
    "        # 从LLM回复中提取评分，这里假设回复格式是比较规范的，包含了评分数字，实际可能需要更严谨的解析处理\n",
    "        score_text = response.choices[0].text.strip()\n",
    "        try:\n",
    "            score = float(score_text)\n",
    "            scores.append(score)\n",
    "            weights.append(weight)\n",
    "            eval = response.choices[1].text.strip()\n",
    "        except ValueError:\n",
    "            print(f\"无法从回复中解析出子问题 '{sub_question}' 的有效评分，回复内容为: {score_text}\")\n",
    "            scores.append(0)\n",
    "            weights.append(weight)\n",
    "\n",
    "    # 根据权重计算综合得分\n",
    "    total_score = sum(s * w for s, w in zip(scores, weights))\n",
    "    return total_score, eval\n",
    "\n",
    "\n",
    "def iterative_evaluation(sub_questions, total_answer, goal=0.9):\n",
    "    \"\"\"\n",
    "    迭代评估子问题答案的主函数，如果得分小于指定目标值（默认为0.9）就继续迭代评估\n",
    "    \"\"\"\n",
    "    score, eval = evaluate_answers(sub_questions, total_answer)\n",
    "    iteration_count = 0\n",
    "    while score < goal:\n",
    "        print(f\"当前得分: {score}，小于{goal}，开始第 {iteration_count + 1} 次迭代评估...\")\n",
    "        score = evaluate_answers(sub_questions, total_answer, eval)\n",
    "        iteration_count += 1\n",
    "    print(f\"最终得分: {score}，满足要求，结束评估。\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def answer(query,context):\n",
    "#     prompt = PromptTemplate(\n",
    "#     input_variables=[\"query, contexts\"],\n",
    "#     template=\"\"\"\n",
    "#     你是一个智能体助手，我给你一个query和一些相关的上下文contexts,由于上下文过长，你需要从中提炼出对于解答query相关的，直接的，有用的信息。\n",
    "#     query: {query}\n",
    "#     contexts: {contexts}\n",
    "#     首先你需要对query进行分析，明确query的意图，以及分析出解答query所需要的可能的信息。\n",
    "#     然后需要对contexts进行分析，提炼出所有有用的信息，最终输出一个优化的上下文，用来解答query。输出的结果需要包含三元组和context，确保这些三元组和context是与query相关的。\n",
    "#     你需要输出三元组及其对三元组的解释，以及context。\n",
    "#     你只需要提炼出与query相关的信息！！！尽量保持原文信息一致性，不用描述和概括。\n",
    "#     不允许出现英文回答，只能包含英文。\n",
    "#     你的输出必须符合下述格式！不允许输出别的内容。\n",
    "#     请一句一句分析，不能跳过任何一句，一步一步思考。\n",
    "#     输出格式如下：\n",
    "#     Triplets:\n",
    "#     (node1,relation,node2)，后面是对形成三元组和解答query有用的信息的解释。\n",
    "#     Context: \n",
    "#     优化提炼之后的上下文。\n",
    "#     请开始提炼:\n",
    "#     \"\"\"\n",
    "#     )\n",
    "#     input = {\n",
    "#         'query': query,\n",
    "#         'contexts': context\n",
    "#     }\n",
    "#     chain = LLMChain(llm=llm, prompt=prompt)\n",
    "#     output = chain.run(input)\n",
    "#     print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "query= df['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求rag系统平均时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "# Combine all contexts into a single string\n",
    "#final_context = \"\\n\\n\".join(all_contexts)\n",
    "#print(final_context)\n",
    "\n",
    "# Create prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "根据以下提供的Triplets,context的信息来回答问题：\n",
    "information:{context}\n",
    "根据上述上下文回答问题：\n",
    "query：{question}。\n",
    "首先分析问题，明确解答问题的信息，然后在提供的information中寻找答案，请一步一步思考。\n",
    "答案不要分段和分点！！！\n",
    "必须完整的回答问题，包括query中可能的多个问题！\n",
    "答案必须和query高度相关！确保是能够回答问题的最直接、准确、明了的答案。\n",
    "不要为你的答案提供理由！！！\n",
    "不要提供上下文信息中未提到的信息，不要胡编乱造。\n",
    "请提供与解决问题最直接相关的答案，不需要推理过程！！！\n",
    "不要说“根据上下文”或“在上下文中提到”或类似的话。\n",
    "你只需要生成回答的答案，不能出现别的无关的语句！！！\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "total_time = 0\n",
    "query_count = 0\n",
    "for q in query:\n",
    "    start_time = time.time() \n",
    "    sub_q = rewrite_and_split_query(q)\n",
    "    triplex,chunks = query_database(sub_q)\n",
    "    triples = \"\\n\".join(triplex)\n",
    "    text = ''\n",
    "    for content in chunks:\n",
    "        text += str(content)\n",
    "    combined_context = f\"Triplets:\\n{triplex}\\n\\nContext:\\n{text}\"\n",
    "    final_context = optimize_contexts(q, combined_context)\n",
    "    prompt = prompt_template.format(context=final_context, question=q)\n",
    "    model = llm \n",
    "    response_text = model.predict(prompt)\n",
    "    iterative_evaluation(sub_questions=sub_q, total_answer=response_text, goal=goal)\n",
    "    end_time = time.time()  # 记录结束时间\n",
    "    elapsed_time = end_time - start_time  # 计算单次查询的时间\n",
    "    total_time += elapsed_time\n",
    "    query_count += 1\n",
    "\n",
    "    # print(f\"Query: {q}\")\n",
    "    # print(f\"Response: {response_text}\")\n",
    "    print(f\"Time taken: {elapsed_time:.4f} seconds\\n\")\n",
    "\n",
    "average_time = total_time / query_count  # 计算平均时间\n",
    "print(f\"Average time taken per query: {average_time:.4f} seconds\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "all_contexts = []\n",
    "\n",
    "# Loop through each query, retrieve context, and store it\n",
    "for q in query:\n",
    "    sub_querys =rewrite_and_split_query(q)\n",
    "    tripes, chunks = query_database(sub_querys)\n",
    "    triples = \"\\n\".join(tripes)\n",
    "    text = ''\n",
    "    for content in chunks:\n",
    "        text += str(content)\n",
    "    combined_context = f\"Triplets:\\n{tripes}\\n\\nContext:\\n{text}\"\n",
    "    final_context = optimize_contexts(q, combined_context)\n",
    "    all_contexts.append(final_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for i, q in enumerate(query):\n",
    "    prompt = prompt_template.format(context=all_contexts[i], question=q)\n",
    "    model = llm \n",
    "    response_text = model.predict(prompt)\n",
    "    responses.append(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = responses\n",
    "df[\"retrival_contexts\"] = all_contexts\n",
    "print(\"Existing columns:\", df.columns)\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KGQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
