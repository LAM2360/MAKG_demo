{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Chunking <a id=\"SemanticChunking\"></a>\n",
    "\n",
    "为什么我们在处理文本时通常会使用固定的分块大小，而不考虑实际内容的语义意义。是不是可以基于文本的语义实现一种更好的方法来处理文本分块，即并非固定参数(**chunksize**)，而是基于语义自行动态确定参数。\n",
    "\n",
    "我们可以通过**embedding**技术进行动态规划\n",
    "\n",
    "**Embedding**将文本转化为高维空间中的向量的技术，这些向量能够反映出文本的语义内容。通过文本嵌入技术，可以捕捉到文本的深层次语义信息。当比较两段文本的嵌入向量时，可以根据它们在高维空间中的距离或者角度，来推断这两段文本在语义上的相似度或者差异。利用相似度，将语义上相似的文本自动分组在一起，形成聚类，这有助于更好地理解和组织大量的文本数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file_path.txt') as file:\n",
    "    essay = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们需要将文本进行拆分，拆分成多个单句，可以按照标点符号进行切分\n",
    "\n",
    "split_char = ['.', '?', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Splitting the essay on '.', '?', and '!'\n",
    "single_sentences_list = re.split(r'[。\\n]+', essay)\n",
    "print (f\"{len(single_sentences_list)} senteneces were found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sentences_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 0\n",
    "for list in single_sentences_list:\n",
    "    length = max(length, len(list))\n",
    "length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要为单个句子拼接更多的句子，但是 `list` 添加比较困难。因此将其转换为字典列表（`List[dict]`）\n",
    "\n",
    "{ 'sentence' : XXX  , 'index' : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentences(sentences, buffer_size=2):\n",
    "    # \n",
    "    \n",
    "    combined_sentences = [\n",
    "        ' '.join(sentences[j]['sentence'] for j in range(max(i - buffer_size, 0), min(i + buffer_size + 1, len(sentences))))\n",
    "        for i in range(len(sentences))\n",
    "    ]   \n",
    "    # 更新原始字典列表，添加组合后的句子\n",
    "    for i, combined_sentence in enumerate(combined_sentences):\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = combine_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来使用**embedding model**对**sentences** 进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "# 加载到GPU\n",
    "import os\n",
    "model = SentenceTransformer(model_name_or_path='your model path')\n",
    "\n",
    "\n",
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    combined_sentences = [\n",
    "        ' '.join(sentences[j]['sentence'] for j in range(max(i - buffer_size, 0), min(i + buffer_size + 1, len(sentences))))\n",
    "        for i in range(len(sentences))\n",
    "    ]   \n",
    "    # 更新原始字典列表，添加组合后的句子\n",
    "    for i, combined_sentence in enumerate(combined_sentences):\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode([x['combined_sentence'] for x in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将embedding添加到sentence中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    sentence['combined_sentence_embedding'] = embeddings[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来需要根据余弦相似度进行切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(sentences[0]['combined_sentence_embedding'], sentences[1]['combined_sentence_embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(embedding_current, embedding_next)\n",
    "        # Convert to cosine distance\n",
    "        distance = 1 - similarity\n",
    "        distances.append(distance)\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "    return distances, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, sentences = calculate_cosine_distances(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[-2]['distance_to_next']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(distances);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.plot(distances)\n",
    "\n",
    "y_upper_bound = 0.25\n",
    "plt.ylim(0, y_upper_bound)\n",
    "plt.xlim(0, len(distances))\n",
    "\n",
    "\n",
    "# We need to get the distance threshold that we'll consider an outlier\n",
    "# We'll use numpy .percentile() for this\n",
    "breakpoint_percentile_threshold = 50\n",
    "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
    "plt.axhline(y=breakpoint_distance_threshold, color='r', linestyle='-')\n",
    "num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) # The amount of distances above your threshold\n",
    "plt.text(x=(len(distances)*.01), y=y_upper_bound/50, s=f\"{num_distances_above_theshold + 1} Chunks\")\n",
    "\n",
    "# Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
    "\n",
    "# Start of the shading and text\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "for i, breakpoint_index in enumerate(indices_above_thresh):\n",
    "    start_index = 0 if i == 0 else indices_above_thresh[i - 1]\n",
    "    end_index = breakpoint_index if i <= len(indices_above_thresh) - 1 else len(distances)\n",
    "\n",
    "    plt.axvspan(start_index, end_index, facecolor=colors[i % len(colors)], alpha=0.25)\n",
    "    plt.text(x=np.average([start_index, end_index]),\n",
    "            y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
    "            s=f\"Chunk #{i}\", horizontalalignment='center',\n",
    "            rotation='vertical')\n",
    "# # Additional step to shade from the last breakpoint to the end of the dataset\n",
    "if indices_above_thresh:\n",
    "    last_breakpoint = indices_above_thresh[-1]\n",
    "    if last_breakpoint < len(distances):\n",
    "        plt.axvspan(last_breakpoint, len(distances), facecolor=colors[len(indices_above_thresh) % len(colors)], alpha=0.25)\n",
    "        plt.text(x=np.average([last_breakpoint, len(distances)]),\n",
    "                 y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
    "                 s=f\"Chunk #{i+1}\",\n",
    "                 rotation='vertical')\n",
    "plt.title(\"Essay Chunks Based On Embedding Breakpoints\")\n",
    "plt.xlabel(\"Index of sentences in essay (Sentence Position)\")\n",
    "plt.ylabel(\"Cosine distance between sequential sentences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# Create a list to hold the grouped sentences\n",
    "chunks = []\n",
    "\n",
    "# Iterate through the breakpoints to slice the sentences\n",
    "for index in indices_above_thresh:\n",
    "    # The end index is the current breakpoint\n",
    "    end_index = index\n",
    "\n",
    "    # Slice the sentence_dicts from the current start index to the end index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    \n",
    "    # Update the start index for the next group\n",
    "    start_index = index + 1\n",
    "\n",
    "# The last group, if any sentences remain\n",
    "if start_index < len(sentences):\n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "\n",
    "# grouped_sentences now contains the chunked sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(max(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(\"---\" + chunk)\n",
    "\n",
    "#把chunks写入txt文件\n",
    "with open('chunks.txt', 'w') as f:\n",
    "    for chunk in chunks:\n",
    "        f.write(chunk+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    buffer = 200\n",
    "    print (f\"Chunk #{i}\")\n",
    "    print (chunk[:buffer].strip())\n",
    "    print (\"...\")\n",
    "    print (chunk[-buffer:].strip())\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
